{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c4229c",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfa35ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RVS NITHIN\\anaconda_2021\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cfacdc",
   "metadata": {},
   "source": [
    "# CMUDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3d9dd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('belford', ['B', 'EH1', 'L', 'F', 'ER0', 'D'])\n",
      "('belfry', ['B', 'EH1', 'L', 'F', 'R', 'IY0'])\n",
      "('belgacom', ['B', 'EH1', 'L', 'G', 'AH0', 'K', 'AA0', 'M'])\n",
      "('belgacom', ['B', 'EH1', 'L', 'JH', 'AH0', 'K', 'AA0', 'M'])\n",
      "('belgard', ['B', 'EH0', 'L', 'G', 'AA1', 'R', 'D'])\n",
      "('belgarde', ['B', 'EH0', 'L', 'G', 'AA1', 'R', 'D', 'IY0'])\n",
      "('belge', ['B', 'EH1', 'L', 'JH', 'IY0'])\n",
      "('belger', ['B', 'EH1', 'L', 'G', 'ER0'])\n",
      "('belgian', ['B', 'EH1', 'L', 'JH', 'AH0', 'N'])\n",
      "('belgians', ['B', 'EH1', 'L', 'JH', 'AH0', 'N', 'Z'])\n",
      "('belgique', ['B', 'EH0', 'L', 'ZH', 'IY1', 'K'])\n",
      "(\"belgique's\", ['B', 'EH0', 'L', 'JH', 'IY1', 'K', 'S'])\n",
      "('belgium', ['B', 'EH1', 'L', 'JH', 'AH0', 'M'])\n",
      "(\"belgium's\", ['B', 'EH1', 'L', 'JH', 'AH0', 'M', 'Z'])\n",
      "('belgo', ['B', 'EH1', 'L', 'G', 'OW2'])\n",
      "('belgrade', ['B', 'EH1', 'L', 'G', 'R', 'EY0', 'D'])\n",
      "('belgrade', ['B', 'EH1', 'L', 'G', 'R', 'AA2', 'D'])\n",
      "(\"belgrade's\", ['B', 'EH1', 'L', 'G', 'R', 'EY0', 'D', 'Z'])\n",
      "(\"belgrade's\", ['B', 'EH1', 'L', 'G', 'R', 'AA2', 'D', 'Z'])\n",
      "('belgrave', ['B', 'EH1', 'L', 'G', 'R', 'EY2', 'V'])\n",
      "('beli', ['B', 'EH1', 'L', 'IY0'])\n",
      "('belich', ['B', 'EH1', 'L', 'IH0', 'K'])\n",
      "('belie', ['B', 'IH0', 'L', 'AY1'])\n",
      "('belied', ['B', 'IH0', 'L', 'AY1', 'D'])\n",
      "('belief', ['B', 'IH0', 'L', 'IY1', 'F'])\n"
     ]
    }
   ],
   "source": [
    "entries = nltk.corpus.cmudict.entries()\n",
    "len(entries)\n",
    "for entry in entries[10000:10025]:\n",
    "    print(entry)\n",
    "#mapping for ipa and english"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4deebc",
   "metadata": {},
   "source": [
    "# Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3b9155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar')\n",
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f4a3c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('good.n.01'),\n",
       " Synset('good.n.02'),\n",
       " Synset('good.n.03'),\n",
       " Synset('commodity.n.01'),\n",
       " Synset('good.a.01'),\n",
       " Synset('full.s.06'),\n",
       " Synset('good.a.03'),\n",
       " Synset('estimable.s.02'),\n",
       " Synset('beneficial.s.01'),\n",
       " Synset('good.s.06'),\n",
       " Synset('good.s.07'),\n",
       " Synset('adept.s.01'),\n",
       " Synset('good.s.09'),\n",
       " Synset('dear.s.02'),\n",
       " Synset('dependable.s.04'),\n",
       " Synset('good.s.12'),\n",
       " Synset('good.s.13'),\n",
       " Synset('effective.s.04'),\n",
       " Synset('good.s.15'),\n",
       " Synset('good.s.16'),\n",
       " Synset('good.s.17'),\n",
       " Synset('good.s.18'),\n",
       " Synset('good.s.19'),\n",
       " Synset('good.s.20'),\n",
       " Synset('good.s.21'),\n",
       " Synset('well.r.01'),\n",
       " Synset('thoroughly.r.02')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4b512",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4375e036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sing'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmerporter = PorterStemmer()\n",
    "stemmerporter.stem('singing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e56e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sing'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "LancasterStemmer().stem('singing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d91587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pass substring to find and remove\n",
    "from nltk.stem import RegexpStemmer\n",
    "stemmerregexp=RegexpStemmer('ing')\n",
    "stemmerregexp.stem('singing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32489b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5539a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mang'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frenchstemmer = SnowballStemmer('french')\n",
    "frenchstemmer.stem('manges')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3540964c",
   "metadata": {},
   "source": [
    "# Pipeline (Sentence tokenizer, Word tokenizer, POS Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e417248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline - has little modules for text-preprocessing\n",
    "#get text-> sentence tokenize, word tokenzier, pos tagging\n",
    "\n",
    "texts = \"\"\"Natural Language Processing (NLP) is an intricate field focused on the challenge of understanding human language. One of its core aspects is handling ‘stop words’ – words which, due to their high frequency in text, often don’t offer significant insights on their own.\n",
    "Stop words like ‘the’, ‘and’, and ‘I’, although common, don’t usually provide meaningful information about a document’s specific topic. By eliminating these words from a corpus, we can more easily identify unique and relevant terms.\n",
    "It’s important to note that there isn’t a universally accepted list of stop words in NLP. However, the Natural Language Toolkit, or NLTK, does offer a list for researchers and practitioners to utilize.\n",
    "Throughout this guide, you’ll discover how to efficiently remove stop words using the nltk module, streamlining your text data for better analysis.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea3391b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing (NLP) is an intricate field focused on the challenge of understanding human language.', 'One of its core aspects is handling ‘stop words’ – words which, due to their high frequency in text, often don’t offer significant insights on their own.', 'Stop words like ‘the’, ‘and’, and ‘I’, although common, don’t usually provide meaningful information about a document’s specific topic.', 'By eliminating these words from a corpus, we can more easily identify unique and relevant terms.', 'It’s important to note that there isn’t a universally accepted list of stop words in NLP.', 'However, the Natural Language Toolkit, or NLTK, does offer a list for researchers and practitioners to utilize.', 'Throughout this guide, you’ll discover how to efficiently remove stop words using the nltk module, streamlining your text data for better analysis.']\n"
     ]
    }
   ],
   "source": [
    "#Sentence tokenizer\n",
    "sentences = nltk.sent_tokenize(texts)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df22bca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: Natural Language Processing (NLP) is an intricate field focused on the challenge of understanding human language.\n",
      "word tokenized: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'an', 'intricate', 'field', 'focused', 'on', 'the', 'challenge', 'of', 'understanding', 'human', 'language', '.']\n",
      "POS tagging: [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('an', 'DT'), ('intricate', 'JJ'), ('field', 'NN'), ('focused', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('challenge', 'NN'), ('of', 'IN'), ('understanding', 'VBG'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "sentence: One of its core aspects is handling ‘stop words’ – words which, due to their high frequency in text, often don’t offer significant insights on their own.\n",
      "word tokenized: ['One', 'of', 'its', 'core', 'aspects', 'is', 'handling', '‘', 'stop', 'words', '’', '–', 'words', 'which', ',', 'due', 'to', 'their', 'high', 'frequency', 'in', 'text', ',', 'often', 'don', '’', 't', 'offer', 'significant', 'insights', 'on', 'their', 'own', '.']\n",
      "POS tagging: [('One', 'CD'), ('of', 'IN'), ('its', 'PRP$'), ('core', 'NN'), ('aspects', 'NNS'), ('is', 'VBZ'), ('handling', 'VBG'), ('‘', 'JJ'), ('stop', 'NN'), ('words', 'NNS'), ('’', 'VBP'), ('–', 'NN'), ('words', 'NNS'), ('which', 'WDT'), (',', ','), ('due', 'JJ'), ('to', 'TO'), ('their', 'PRP$'), ('high', 'JJ'), ('frequency', 'NN'), ('in', 'IN'), ('text', 'NN'), (',', ','), ('often', 'RB'), ('don', 'VBZ'), ('’', 'JJ'), ('t', 'NN'), ('offer', 'NN'), ('significant', 'JJ'), ('insights', 'NNS'), ('on', 'IN'), ('their', 'PRP$'), ('own', 'JJ'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "sentence: Stop words like ‘the’, ‘and’, and ‘I’, although common, don’t usually provide meaningful information about a document’s specific topic.\n",
      "word tokenized: ['Stop', 'words', 'like', '‘', 'the', '’', ',', '‘', 'and', '’', ',', 'and', '‘', 'I', '’', ',', 'although', 'common', ',', 'don', '’', 't', 'usually', 'provide', 'meaningful', 'information', 'about', 'a', 'document', '’', 's', 'specific', 'topic', '.']\n",
      "POS tagging: [('Stop', 'NNP'), ('words', 'NNS'), ('like', 'IN'), ('‘', 'VBP'), ('the', 'DT'), ('’', 'NN'), (',', ','), ('‘', 'NNP'), ('and', 'CC'), ('’', 'NNP'), (',', ','), ('and', 'CC'), ('‘', 'NNP'), ('I', 'PRP'), ('’', 'VBP'), (',', ','), ('although', 'IN'), ('common', 'JJ'), (',', ','), ('don', 'JJ'), ('’', 'FW'), ('t', 'NN'), ('usually', 'RB'), ('provide', 'VBP'), ('meaningful', 'JJ'), ('information', 'NN'), ('about', 'IN'), ('a', 'DT'), ('document', 'NN'), ('’', 'NN'), ('s', 'NN'), ('specific', 'JJ'), ('topic', 'NN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "sentence: By eliminating these words from a corpus, we can more easily identify unique and relevant terms.\n",
      "word tokenized: ['By', 'eliminating', 'these', 'words', 'from', 'a', 'corpus', ',', 'we', 'can', 'more', 'easily', 'identify', 'unique', 'and', 'relevant', 'terms', '.']\n",
      "POS tagging: [('By', 'IN'), ('eliminating', 'VBG'), ('these', 'DT'), ('words', 'NNS'), ('from', 'IN'), ('a', 'DT'), ('corpus', 'NN'), (',', ','), ('we', 'PRP'), ('can', 'MD'), ('more', 'RBR'), ('easily', 'RB'), ('identify', 'VB'), ('unique', 'JJ'), ('and', 'CC'), ('relevant', 'JJ'), ('terms', 'NNS'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "sentence: It’s important to note that there isn’t a universally accepted list of stop words in NLP.\n",
      "word tokenized: ['It', '’', 's', 'important', 'to', 'note', 'that', 'there', 'isn', '’', 't', 'a', 'universally', 'accepted', 'list', 'of', 'stop', 'words', 'in', 'NLP', '.']\n",
      "POS tagging: [('It', 'PRP'), ('’', 'VBZ'), ('s', 'JJ'), ('important', 'JJ'), ('to', 'TO'), ('note', 'VB'), ('that', 'IN'), ('there', 'EX'), ('isn', 'JJ'), ('’', 'NNP'), ('t', 'NN'), ('a', 'DT'), ('universally', 'RB'), ('accepted', 'JJ'), ('list', 'NN'), ('of', 'IN'), ('stop', 'JJ'), ('words', 'NNS'), ('in', 'IN'), ('NLP', 'NNP'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "sentence: However, the Natural Language Toolkit, or NLTK, does offer a list for researchers and practitioners to utilize.\n",
      "word tokenized: ['However', ',', 'the', 'Natural', 'Language', 'Toolkit', ',', 'or', 'NLTK', ',', 'does', 'offer', 'a', 'list', 'for', 'researchers', 'and', 'practitioners', 'to', 'utilize', '.']\n",
      "POS tagging: [('However', 'RB'), (',', ','), ('the', 'DT'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Toolkit', 'NNP'), (',', ','), ('or', 'CC'), ('NLTK', 'NNP'), (',', ','), ('does', 'VBZ'), ('offer', 'VB'), ('a', 'DT'), ('list', 'NN'), ('for', 'IN'), ('researchers', 'NNS'), ('and', 'CC'), ('practitioners', 'NNS'), ('to', 'TO'), ('utilize', 'VB'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "sentence: Throughout this guide, you’ll discover how to efficiently remove stop words using the nltk module, streamlining your text data for better analysis.\n",
      "word tokenized: ['Throughout', 'this', 'guide', ',', 'you', '’', 'll', 'discover', 'how', 'to', 'efficiently', 'remove', 'stop', 'words', 'using', 'the', 'nltk', 'module', ',', 'streamlining', 'your', 'text', 'data', 'for', 'better', 'analysis', '.']\n",
      "POS tagging: [('Throughout', 'IN'), ('this', 'DT'), ('guide', 'NN'), (',', ','), ('you', 'PRP'), ('’', 'VBP'), ('ll', 'JJ'), ('discover', 'VB'), ('how', 'WRB'), ('to', 'TO'), ('efficiently', 'RB'), ('remove', 'VB'), ('stop', 'NN'), ('words', 'NNS'), ('using', 'VBG'), ('the', 'DT'), ('nltk', 'NN'), ('module', 'NN'), (',', ','), ('streamlining', 'VBG'), ('your', 'PRP$'), ('text', 'NN'), ('data', 'NNS'), ('for', 'IN'), ('better', 'JJR'), ('analysis', 'NN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# word tokenizer + POS Tagging\n",
    "for sentence in sentences:\n",
    "    print(\"sentence:\",sentence)\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(\"word tokenized:\",words)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    print(\"POS tagging:\",tagged)\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
